{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_hota(pred_boxes, gt_boxes, iou_threshold=0.5, c_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Higher Order Tracking Accuracy (HOTA) metric.\n",
    "    \n",
    "    Args:\n",
    "        pred_boxes (list): List of predicted bounding boxes.\n",
    "        gt_boxes (list): List of ground truth bounding boxes.\n",
    "        iou_threshold (float): Intersection over Union (IoU) threshold to consider a match.\n",
    "        c_threshold (float): Confidence threshold for predicted boxes.\n",
    "    \n",
    "    Returns:\n",
    "        float: HOTA score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort predicted boxes by confidence scores in descending order\n",
    "    pred_boxes = sorted(pred_boxes, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Initialize variables for HOTA calculation\n",
    "    HOTA = 0.0\n",
    "    DetA = 0.0\n",
    "    HOTAL = 0.0\n",
    "    HOTAL_empty = 0.0\n",
    "    mme = 0.0\n",
    "    \n",
    "    # Create boolean arrays to track matched predictions and ground truth boxes\n",
    "    pred_matched = np.zeros(len(pred_boxes), dtype=bool)\n",
    "    gt_matched = np.zeros(len(gt_boxes), dtype=bool)\n",
    "    \n",
    "    # Iterate over predicted boxes\n",
    "    for p, pred_box in enumerate(pred_boxes):\n",
    "        # Filter out low-confidence predictions\n",
    "        if pred_box['confidence'] < c_threshold:\n",
    "            break\n",
    "        \n",
    "        # Initialize variables for HOTA calculation per prediction\n",
    "        best_iou = -np.inf\n",
    "        best_match = -1\n",
    "        \n",
    "        # Iterate over ground truth boxes\n",
    "        for g, gt_box in enumerate(gt_boxes):\n",
    "            # Skip already matched ground truth boxes\n",
    "            if gt_matched[g]:\n",
    "                continue\n",
    "            \n",
    "            # Compute IoU between predicted and ground truth boxes\n",
    "            iou = calculate_iou(pred_box['bbox'], gt_box['bbox'])\n",
    "            \n",
    "            # Check if IoU is higher than the threshold and better than previous matches\n",
    "            if iou > iou_threshold and iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = g\n",
    "        \n",
    "        # Update match status for the best match (if any)\n",
    "        if best_match != -1:\n",
    "            pred_matched[p] = True\n",
    "            gt_matched[best_match] = True\n",
    "            \n",
    "            # Calculate HOTA components for the matched prediction\n",
    "            HOTA += best_iou\n",
    "            DetA += 1\n",
    "            HOTAL += 1\n",
    "        else:\n",
    "            # Calculate HOTA components for the unmatched prediction\n",
    "            HOTAL_empty += 1\n",
    "    \n",
    "    # Calculate HOTA components for unmatched ground truth boxes\n",
    "    HOTAL += len(gt_boxes) - np.count_nonzero(gt_matched)\n",
    "    \n",
    "    # Calculate the MME (Multiple Matches Error) component\n",
    "    mme = np.count_nonzero(pred_matched) - DetA\n",
    "    \n",
    "    # Calculate the HOTA score\n",
    "    HOTA_score = HOTA / (DetA + HOTAL + HOTAL_empty + mme)\n",
    "    \n",
    "    return [HOTA_score, DetA]\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        box1 (tuple): Bounding box coordinates of the format (x1, y1, x2, y2).\n",
    "        box2 (tuple): Bounding box coordinates of the format (x1, y1, x2, y2).\n",
    "    \n",
    "    Returns:\n",
    "        float: IoU score.\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    iou = intersection / float(area_box1 + area_box2 - intersection)\n",
    "    \n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOTA score: [0.3981818181818182, 3.0]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "pred_boxes = [\n",
    "    {'bbox': (70, 50, 150, 150), 'confidence': 0.8},\n",
    "    {'bbox': (115, 120, 200, 200), 'confidence': 0.9},\n",
    "    {'bbox': (100, 100, 210, 200), 'confidence': 0.9},\n",
    "    # More predicted boxes...\n",
    "]\n",
    "\n",
    "gt_boxes = [\n",
    "    {'bbox': (50, 50, 150, 150)},\n",
    "    {'bbox': (100, 100, 200, 200)},\n",
    "    {'bbox': (100, 100, 200, 200)},\n",
    "    # More ground truth boxes...\n",
    "]\n",
    "\n",
    "hota_score = calculate_hota(pred_boxes, gt_boxes)\n",
    "print(\"HOTA score:\", hota_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (4,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m gt_box \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(gt_box)[\u001b[39m1\u001b[39m:]\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(pred_box\u001b[39m.\u001b[39mshape, gt_box\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 27\u001b[0m distances \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39;49mdistances\u001b[39m.\u001b[39;49miou_matrix(\u001b[39mlist\u001b[39;49m(pred_box), \u001b[39mlist\u001b[39;49m(gt_box), max_iou\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(distances))\n\u001b[1;32m     30\u001b[0m acc\u001b[39m.\u001b[39mupdate(pred_box, gt_box, distances)\n",
      "File \u001b[0;32m~/Projects/.env/lib/python3.10/site-packages/motmetrics/distances.py:119\u001b[0m, in \u001b[0;36miou_matrix\u001b[0;34m(objs, hyps, max_iou)\u001b[0m\n\u001b[1;32m    117\u001b[0m objs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masfarray(objs)\n\u001b[1;32m    118\u001b[0m hyps \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masfarray(hyps)\n\u001b[0;32m--> 119\u001b[0m \u001b[39massert\u001b[39;00m objs\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39massert\u001b[39;00m hyps\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    121\u001b[0m iou \u001b[39m=\u001b[39m boxiou(objs[:, \u001b[39mNone\u001b[39;00m], hyps[\u001b[39mNone\u001b[39;00m, :])\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "\n",
    "pred_boxes = [\n",
    "    [1, 50, 50, 100, 100, 0.8],\n",
    "    [2, 100, 100, 100, 100, 0.9],\n",
    "    # More predicted boxes...\n",
    "]\n",
    "\n",
    "gt_boxes = [\n",
    "    [1, 50, 50, 100, 100],\n",
    "    [2, 100, 100, 100, 100],\n",
    "    # More ground truth boxes...\n",
    "]\n",
    "\n",
    "mh = mm.metrics.create()\n",
    "\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Create distance matrix\n",
    "\n",
    "\n",
    "for pred_box, gt_box in zip(pred_boxes, gt_boxes,):\n",
    "    pred_box = np.array(pred_box)[1:5]\n",
    "    gt_box = np.array(gt_box)[1:]\n",
    "    print(pred_box.shape, gt_box.shape)\n",
    "    distances = mm.distances.iou_matrix(list(pred_box), list(gt_box), max_iou=0.5)\n",
    "    print(type(distances))\n",
    "    \n",
    "    acc.update(pred_box, gt_box, distances)\n",
    "\n",
    "metrics = mm.metrics.motchallenge_metrics\n",
    "summary = mh.compute(acc, metrics)\n",
    "\n",
    "hota_score = summary['HOTA'].global_track\n",
    "\n",
    "print(\"HOTA score:\", hota_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motMetricsEnhancedCalculator(gtSource, tSource):\n",
    "  # import required packages\n",
    "  import motmetrics as mm\n",
    "  import numpy as np\n",
    "  \n",
    "  # load ground truth\n",
    "  gt = np.loadtxt(gtSource, delimiter=' ')\n",
    "\n",
    "  # load tracking output\n",
    "  t = np.loadtxt(tSource, delimiter=' ')\n",
    "\n",
    "  # Create an accumulator that will be updated during each frame\n",
    "  acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "  # Max frame number maybe different for gt and t files\n",
    "  for frame in range(int(gt[:,0].max())):\n",
    "    frame += 1 # detection and frame numbers begin at 1\n",
    "\n",
    "    # select id, x, y, width, height for current frame\n",
    "    # required format for distance calculation is X, Y, Width, Height \\\n",
    "    # We already have this format\n",
    "    gt_dets = gt[gt[:,0]==frame,1:6] # select all detections in gt\n",
    "    t_dets = t[t[:,0]==frame,1:6] # select all detections in t\n",
    "\n",
    "    C = mm.distances.iou_matrix(gt_dets[:,1:], t_dets[:,1:], \\\n",
    "                                max_iou=0.5) # format: gt, t\n",
    "\n",
    "    # Call update once for per frame.\n",
    "    # format: gt object ids, t object ids, distance\n",
    "    acc.update(gt_dets[:,0].astype('int').tolist(), \\\n",
    "              t_dets[:,0].astype('int').tolist(), C)\n",
    "\n",
    "  mh = mm.metrics.create()\n",
    "\n",
    "  summary = mh.compute(acc, metrics=['num_frames', 'idf1', 'idp', 'idr', \\\n",
    "                                     'recall', 'precision', 'num_objects', \\\n",
    "                                     'mostly_tracked', 'partially_tracked', \\\n",
    "                                     'mostly_lost', 'num_false_positives', \\\n",
    "                                     'num_misses', 'num_switches', \\\n",
    "                                     'num_fragmentations', 'mota', 'motp' \\\n",
    "                                    ], \\\n",
    "                      name='acc')\n",
    "\n",
    "  strsummary = mm.io.render_summary(\n",
    "      summary,\n",
    "      #formatters={'mota' : '{:.2%}'.format},\n",
    "      namemap={'idf1': 'IDF1', 'idp': 'IDP', 'idr': 'IDR', 'recall': 'Rcll', \\\n",
    "               'precision': 'Prcn', 'num_objects': 'GT', \\\n",
    "               'mostly_tracked' : 'MT', 'partially_tracked': 'PT', \\\n",
    "               'mostly_lost' : 'ML', 'num_false_positives': 'FP', \\\n",
    "               'num_misses': 'FN', 'num_switches' : 'IDsw', \\\n",
    "               'num_fragmentations' : 'FM', 'mota': 'MOTA', 'motp' : 'MOTP',  \\\n",
    "              }\n",
    "  )\n",
    "  print(strsummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn    GT  MT  PT  ML  FP  FN  IDsw  FM      MOTA      MOTP\n",
      "acc         200  0.786342  0.797668  0.775333  0.945333  0.972565  1500  10   0   0  40  82    23  12  0.903333  0.089832\n"
     ]
    }
   ],
   "source": [
    "motMetricsEnhancedCalculator('data/anno_test_track/2020_12_16_Montana_at_Washington_gt_xywh.txt', 'hota/2020_12_16_Montana_at_Washington.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     num_frames      IDF1       IDP       IDR      Rcll      Prcn    GT  MT  PT  ML    FP    FN  IDsw  FM      MOTA      MOTP\n",
      "acc         193  0.025357  0.023278  0.027842  0.059165  0.049467  1724   0   0  10  1960  1622    32  39 -1.096288  0.396978\n"
     ]
    }
   ],
   "source": [
    "motMetricsEnhancedCalculator('data/anno_test_track/2021_01_16_Citadel_at_VirginiaMilitary_gt.txt', 'hota/2021_01_16_Citadel_at_VirginiaMilitary.txt')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пробуем посчитать метрику HOTA с использованием библиотеки Trackeval.(!разобраться с форматом подаваемых данных!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /home/skorp/Projects/.env/lib/python3.10/site-packages/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : True                          \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'similarity_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 28\u001b[0m\n\u001b[1;32m     12\u001b[0m hota \u001b[39m=\u001b[39m trackeval\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mHOTA()\n\u001b[1;32m     14\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnum_tracker_dets\u001b[39m\u001b[39m'\u001b[39m:[\n\u001b[1;32m     15\u001b[0m     [\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m0.8\u001b[39m],\n\u001b[1;32m     16\u001b[0m     [\u001b[39m2\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m0.9\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mgt_ids\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m],\n\u001b[1;32m     26\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mtracker_ids\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m]}\n\u001b[0;32m---> 28\u001b[0m hota\u001b[39m.\u001b[39;49meval_sequence(data)\n",
      "File \u001b[0;32m~/Projects/.env/lib/python3.10/site-packages/trackeval/_timing.py:17\u001b[0m, in \u001b[0;36mtime.<locals>.wrap\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m DO_TIMING:\n\u001b[1;32m     15\u001b[0m     \u001b[39m# Run function with timing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     ts \u001b[39m=\u001b[39m perf_counter()\n\u001b[0;32m---> 17\u001b[0m     result \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     18\u001b[0m     te \u001b[39m=\u001b[39m perf_counter()\n\u001b[1;32m     19\u001b[0m     tt \u001b[39m=\u001b[39m te\u001b[39m-\u001b[39mts\n",
      "File \u001b[0;32m~/Projects/.env/lib/python3.10/site-packages/trackeval/metrics/hota.py:56\u001b[0m, in \u001b[0;36mHOTA.eval_sequence\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m# First loop through each timestep and accumulate global track information.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m t, (gt_ids_t, tracker_ids_t) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mgt_ids\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mtracker_ids\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     54\u001b[0m     \u001b[39m# Count the potential matches between ids in each timestep\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m# These are normalised, weighted by the match similarity.\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     similarity \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39msimilarity_scores\u001b[39;49m\u001b[39m'\u001b[39;49m][t]\n\u001b[1;32m     57\u001b[0m     sim_iou_denom \u001b[39m=\u001b[39m similarity\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)[np\u001b[39m.\u001b[39mnewaxis, :] \u001b[39m+\u001b[39m similarity\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)[:, np\u001b[39m.\u001b[39mnewaxis] \u001b[39m-\u001b[39m similarity\n\u001b[1;32m     58\u001b[0m     sim_iou \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(similarity)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'similarity_scores'"
     ]
    }
   ],
   "source": [
    "import trackeval\n",
    "import numpy as np\n",
    "\n",
    "gt_data_root = \"data/anno_test_track/2020_12_16_Montana_at_Washington_gt.txt\"\n",
    "pr_data_root = \"hota/2020_12_16_Montana_at_Washington.txt\"\n",
    "\n",
    "evaluator = trackeval.Evaluator()\n",
    "\n",
    "#predicted_data = trackeval.datasets.MotChallenge2DBox(pr_data_root)\n",
    "#groundtruth_data = trackeval.datasets.MotChallenge2DBox(gt_data_root)\n",
    "\n",
    "hota = trackeval.metrics.HOTA()\n",
    "\n",
    "data = {'num_tracker_dets':[\n",
    "    [1, 50, 50, 100, 100, 0.8],\n",
    "    [2, 100, 100, 100, 100, 0.9],\n",
    "  ],\n",
    "  'num_gt_dets': [\n",
    "    [1, 50, 50, 100, 100],\n",
    "    [2, 100, 100, 100, 100],\n",
    "    # More ground truth boxes...\n",
    "  ],\n",
    "  'num_gt_ids': 2,\n",
    "  'num_tracker_ids': 2,\n",
    "  'gt_ids':[1,2],\n",
    "  'tracker_ids': [1,2]}\n",
    "\n",
    "hota.eval_sequence(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
